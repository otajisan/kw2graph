import asyncio

import structlog

from kw2graph import config
from kw2graph.domain.contents_fetcher import ContentsFetcherService
from kw2graph.domain.keywords_analyzer import KeywordsAnalyzerService
from kw2graph.infrastructure.graphdb import GraphDatabaseRepository

from kw2graph.usecase.base import UseCaseBase
from kw2graph.usecase.input.analyze_keywords import AnalyzeKeywordsInput
from kw2graph.usecase.input.get_candidate import GetCandidateInput
from kw2graph.usecase.input.submit_task import SubmitTaskInput
from kw2graph.util.text_formatter import TextFormatter

logger = structlog.get_logger(__name__)


class SubmitGraphAnalysisUseCase(UseCaseBase):
    def __init__(self, settings: config.Settings,
                 graph_repo: GraphDatabaseRepository):
        super().__init__(settings)
        self.graph_repo = graph_repo
        self.formatter = TextFormatter()
        # æ—¢å­˜ã®åˆ†æã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ï¼ˆã“ã“ã§ã¯ãƒªãƒã‚¸ãƒˆãƒªã‚’ç›´æ¥æ³¨å…¥ã›ãšã€ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ï¼‰
        self.fetcher = ContentsFetcherService(settings)
        self.analyzer_service = KeywordsAnalyzerService(settings)

    async def execute(self, in_data: SubmitTaskInput):
        """
        æœ€åˆã®èµ·ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è§£æã¨ç™»éŒ²ã‚’å®Ÿè¡Œã—ã€ãã®å¾Œã€å†å¸°çš„ãªæ¢ç´¢ã‚’ãƒˆãƒªã‚¬ãƒ¼ã—ã¾ã™ã€‚
        """
        logger.info("START: Initial graph analysis task.", keyword=in_data.seed_keyword)

        # 1. æœ€åˆã®èµ·ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡¦ç†ã‚’å®Ÿè¡Œ
        success = await self._process_single_keyword(in_data)

        if success:
            logger.info("Initial analysis successful. Starting recursive search (Depth 1).")

            # 2. ã‚¹ãƒ†ãƒƒãƒ— 2: æœ€åˆã®ãƒ›ãƒƒãƒ—ï¼ˆéšå±¤ 1ï¼‰ã®è‡ªå‹•æ¢ç´¢ã‚’é–‹å§‹
            # å¿…é ˆæ¡ä»¶: entity_type='Proper', score >= 0.90
            await self.execute_recursive_analysis(
                in_data=in_data,
                seed_keyword=in_data.seed_keyword,
                min_score=0.90,
                entity_type='Proper',
                depth=1  # æœ€åˆã®ãƒ›ãƒƒãƒ—
            )

            # 3. ã‚¹ãƒ†ãƒƒãƒ— 3: 2ç•ªç›®ã®ãƒ›ãƒƒãƒ—ï¼ˆéšå±¤ 2ï¼‰ã®è‡ªå‹•æ¢ç´¢ã‚’é–‹å§‹
            # å¿…é ˆæ¡ä»¶: entity_type='Proper', score >= 0.95 (ã‚ˆã‚Šå³ã—ã)
            await self.execute_recursive_analysis(
                in_data=in_data,
                seed_keyword=in_data.seed_keyword,
                min_score=0.95,
                entity_type='Proper',
                depth=2  # 2ç•ªç›®ã®ãƒ›ãƒƒãƒ—
            )

        logger.info("FINISH: All analysis tasks completed.", keyword=in_data.seed_keyword)
        return success

    # -----------------------------------------------------
    # ğŸ’¡ æ–°è¦: å†å¸°çš„ãªæ¢ç´¢ã¨ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚’ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰
    # -----------------------------------------------------
    async def execute_recursive_analysis(self,
                                         in_data: SubmitTaskInput,
                                         seed_keyword: str,
                                         min_score: float,
                                         entity_type: str,
                                         depth: int):
        """
        GraphDBã‹ã‚‰æ¡ä»¶ã«åˆã†æ–°ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç™ºè¦‹ã—ã€ãã‚Œãã‚Œã«å¯¾ã—ã¦è§£æã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        """
        logger.info(f"START: Recursive discovery at Depth {depth}.",
                    source_keyword=seed_keyword, min_score=min_score)

        # 1. GraphDBã‹ã‚‰æ¬¡ã®éšå±¤ã®æ–°ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å€™è£œã‚’å–å¾—
        # Note: Gremlinãƒ¡ã‚½ãƒƒãƒ‰ã¯ depth=1 ã§å‘¼ã³å‡ºã—ã€ç›´è¿‘ã®é–¢é€£ãƒãƒ¼ãƒ‰ã®ã¿ã‚’æ¢ã™
        new_keywords = await self.graph_repo.get_new_and_eligible_keywords(
            seed_keyword=seed_keyword,
            min_score=min_score,
            entity_type=entity_type,
            max_depth=1
        )

        if not new_keywords:
            logger.info(f"No new eligible keywords found at Depth {depth}.")
            return

        logger.info(f"Discovered {len(new_keywords)} new keywords at Depth {depth}. Starting parallel processing.")

        # 2. ç™ºè¦‹ã•ã‚ŒãŸå„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¯¾ã—ã¦ä¸¦åˆ—ã§è§£æã¨ç™»éŒ²ã‚’å®Ÿè¡Œ
        tasks = []
        for new_kw in new_keywords:
            # æ–°ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’èµ·ç‚¹ã¨ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’æº–å‚™
            new_input = SubmitTaskInput(
                seed_keyword=new_kw,
                index=in_data.index,
                field=in_data.field,
                max_titles=50  # ã‚¿ã‚¤ãƒˆãƒ«æ•°ã¯è¨­å®šå€¤ã‚’åˆ©ç”¨
            )
            # å„ã‚¿ã‚¹ã‚¯ã¯ _process_single_keyword ã‚’å®Ÿè¡Œã™ã‚‹
            tasks.append(self._process_single_keyword(new_input))

        # 3. ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—ã§å¾…æ©Ÿ
        await asyncio.gather(*tasks, return_exceptions=False)

        logger.info(f"FINISH: Recursive discovery at Depth {depth} completed.")

    # -----------------------------------------------------
    # ğŸ’¡ æ–°è¦: å˜ä¸€ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è§£æã¨ç™»éŒ²ã‚’å®Ÿè¡Œã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    # -----------------------------------------------------
    async def _process_single_keyword(self, in_data: SubmitTaskInput) -> bool:
        """
        å˜ä¸€ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¯¾ã—ã¦ã€Elasticsearchå–å¾— -> OpenAIè§£æ -> GraphDBç™»éŒ²ã‚’ä¸€è²«ã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚
        """
        logger.debug(f"Processing single keyword: {in_data.seed_keyword}")

        # 1. Elasticsearchã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
        # ... (fetcher.fetch ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯å‰ã®ã‚³ãƒ¼ãƒ‰ã‚’å‚ç…§)
        search_result = await self.fetcher.fetch(GetCandidateInput(
            index=in_data.index,
            field=in_data.field,
            keyword=in_data.seed_keyword,
        ))

        # titles = [c['snippet']['title'] for c in candidates]
        titles = []
        for candidate in search_result.candidates:
            titles.append(candidate['snippet']['title'])

        # 2. å‰å‡¦ç† (æ­£è¦åŒ–ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°)
        normalized_titles = self.formatter.normalize_titles_list(titles)
        if not normalized_titles: return False

        # 3. OpenAIã§éåŒæœŸãƒãƒƒãƒè§£æ
        analyze_output = await self.analyzer_service.analyze(
            in_data=AnalyzeKeywordsInput(seed_keyword=in_data.seed_keyword, children=normalized_titles),
            use_batch=True
        )
        if not analyze_output.results: return False

        # 4. GraphDBã¸ã®ç™»éŒ²
        registration_data = [
            {'keyword': item.keyword, 'score': item.score, 'iab_categories': item.iab_categories,
             'entity_type': item.entity_type}
            for item in analyze_output.results
        ]

        # channel_name ã¯çœç•¥
        channel_name = None

        success = await self.graph_repo.register_related_keywords(
            seed_keyword=in_data.seed_keyword,
            extracted_data=registration_data,
            channel_name=channel_name
        )
        return success
